{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6d12a9",
   "metadata": {},
   "source": [
    "# Problem 3: Collaborative Filtering Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047e012",
   "metadata": {},
   "source": [
    "# Spark ALS based colloborative filtering model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc27d9",
   "metadata": {},
   "source": [
    "Get the dataset from s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97d1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dbfs_dir = 's3://netflixdata-yr/'\n",
    "movies_filename = dbfs_dir + 'movie_titles.txt'\n",
    "testing_filename = dbfs_dir + 'TestingRatings.txt'\n",
    "training_filename = dbfs_dir + 'TrainingRatings.txt'\n",
    "# https://github.com/snehalnair/als-recommender-pyspark/blob/master/Recommendation_Engine_MovieLens.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706a2dd",
   "metadata": {},
   "source": [
    "Define the schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b274678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "movies_df_schema = StructType(\n",
    "  [StructField('movie_Id', IntegerType()),\n",
    "   StructField('year', IntegerType()),\n",
    "   StructField('title', StringType())]\n",
    ")\n",
    "\n",
    "testing_df_schema = StructType(\n",
    "  [StructField('movie_Id', IntegerType()),\n",
    "   StructField('userId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")\n",
    "\n",
    "training_df_schema = StructType(\n",
    "  [StructField('movie_Id', IntegerType()),\n",
    "   StructField('userId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9252",
   "metadata": {},
   "source": [
    "Load and cache the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7c79b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movie_Id: int, userId: int, rating: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "movies_titles_df = sqlContext.read.options(header=False, inferSchema=False).schema(movies_df_schema).csv(\"s3://netflixdata-yr/movie_titles.txt\")\n",
    "testing_df = sqlContext.read.options(header=False, inferSchema=False).schema(testing_df_schema).csv(\"s3://netflixdata-yr/TestingRatings.txt\")\n",
    "training_df = sqlContext.read.options(header=False, inferSchema=False).schema(testing_df_schema).csv(\"s3://netflixdata-yr/TrainingRatings.txt\")\n",
    "\n",
    "movies_titles_df.cache()\n",
    "testing_df.cache()\n",
    "training_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e370710",
   "metadata": {},
   "source": [
    " Merge movie titles dataframe with testing and training dataframe and drop the duplicate column movieID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfe333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_df = movies_titles_df.join(testing_ratings_df, movies_titles_df[\"movie_Id\"]==testing_ratings_df[\"movieId\"])\n",
    "# testing_df = testing_df.drop(\"movieId\")\n",
    "# print(testing_df.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df = movies_titles_df.join(training_ratings_df, movies_titles_df[\"movie_Id\"]==training_ratings_df[\"movieId\"])\n",
    "# training_df = training_df.drop(\"movieId\")\n",
    "# print(training_df.show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8ed11",
   "metadata": {},
   "source": [
    "Creating temp view for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64eedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import pyspark\n",
    "sqlContext = pyspark.SQLContext(sc)  \n",
    "testing_df.createOrReplaceTempView('testing_df')\n",
    "training_df.createOrReplaceTempView('training_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88714a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required functions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03588890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALS"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create & Confirm ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movie_Id\", ratingCol=\"rating\", nonnegative = True, implicitPrefs = False, coldStartStrategy=\"drop\")\n",
    "type(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5543a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  9\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [5,10,15]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, 0.1]) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9303f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidator_60c9bb2617d8\n"
     ]
    }
   ],
   "source": [
    "# Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Confirm cv was built\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8c6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Fit cross validator to the 'train' dataset\n",
    "model = cv.fit(training_df)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc043b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.recommendation.ALSModel'>\n",
      "**Best Model**\n",
      "  Rank: 15\n",
      "  MaxIter: 10\n",
      "  RegParam: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Print best_model\n",
    "print(type(best_model))\n",
    "\n",
    "# Print the best ALS model parameters\n",
    "print(\"**Best Model**\")\n",
    "\n",
    "# # # Print \"Rank\"\n",
    "print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
    "\n",
    "# # Print \"MaxIter\"\n",
    "print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "\n",
    "# # Print \"RegParam\"\n",
    "print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23167f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training data is: 0.767821286433335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8306:===============================================>    (182 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for training data is: 0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_predictions = best_model.transform(training_df)\n",
    "RMSE_train = evaluator.evaluate(train_predictions)\n",
    "print(\"RMSE for training data is:\", RMSE_train)\n",
    "\n",
    "mae_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mae\"})\n",
    "print(\"MAE for training data is: %.3f\" % mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63f3fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for testing data is: 0.8371780505884548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8366:=========================================>          (160 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for testing data is:: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# View the predictions\n",
    "test_predictions = best_model.transform(testing_df)\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(\"RMSE for testing data is:\", RMSE)\n",
    "\n",
    "mae = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mae\"})\n",
    "print(\"MAE for testing data is:: %.3f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b91b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+\n",
      "|movie_Id| userId|rating|prediction|\n",
      "+--------+-------+------+----------+\n",
      "|      28|2358799|   3.0| 3.9407175|\n",
      "|     156| 973051|   5.0| 4.0387187|\n",
      "|     851|1189060|   3.0| 3.4920712|\n",
      "|    1100|2376892|   2.0| 2.2084737|\n",
      "|    1123|1628484|   3.0| 3.4529614|\n",
      "|    1289|1552084|   3.0| 3.5745318|\n",
      "|    1744|2376892|   5.0| 3.7657683|\n",
      "|    1851| 675056|   4.0| 3.5719743|\n",
      "|    1983|2376892|   4.0|  3.174658|\n",
      "|    1983|2629660|   3.0| 2.8910725|\n",
      "+--------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view top 10 test prediction movies\n",
    "test_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72400f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8449:====================================================>(99 + 1) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|  3039|[[6991, 4.479667]...|\n",
      "|  3694|[[12293, 4.332815...|\n",
      "|  4247|[[12293, 4.847693...|\n",
      "|  7601|[[11812, 4.940127...|\n",
      "|  8095|[[6991, 5.582224]...|\n",
      "|  8135|[[12952, 6.137531...|\n",
      "|  9399|[[12952, 5.355542...|\n",
      "| 10897|[[14283, 4.554458...|\n",
      "| 11215|[[12952, 5.475445...|\n",
      "| 11430|[[12952, 6.523889...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Generate n Recommendations for all users and list top 10\n",
    "reco = best_model.recommendForAllUsers(10)\n",
    "reco.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c24fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8527:================================================>    (91 + 8) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|userId|movie_Id|   rating|\n",
      "+------+--------+---------+\n",
      "|   481|    6991| 5.736088|\n",
      "|   481|   14648| 5.140416|\n",
      "|   481|   10743| 5.124085|\n",
      "|   481|   14361| 5.055481|\n",
      "|   481|    7569|5.0513654|\n",
      "|   481|   12952|5.0289598|\n",
      "|   481|     634| 4.975964|\n",
      "|   481|    4238|4.9639053|\n",
      "|   481|   15567|  4.94801|\n",
      "|   481|   10947| 4.940893|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.functions import col, avg, when, count\n",
    "\n",
    "reco1 = reco\\\n",
    "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "    .select('userId', col(\"rec_exp.movie_Id\"), col(\"rec_exp.rating\"))\n",
    "\n",
    "reco1.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c232875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----+--------------------+\n",
      "|movie_Id|userId|   rating|year|               title|\n",
      "+--------+------+---------+----+--------------------+\n",
      "|   14185|    79| 4.634067|1964|        Mary Poppins|\n",
      "|    6287|    79| 4.590269|1990|        Pretty Woman|\n",
      "|    1256|    79|4.5632133|1994|The Best of Frien...|\n",
      "|   14283|    79|4.5457993|1994|The Best of Frien...|\n",
      "|   12952|    79|4.5401516|2005|The God Who Wasn'...|\n",
      "|    4129|    79|4.5198183|1970|Santa Claus Is Co...|\n",
      "|   14648|    79|4.4529114|2003|Finding Nemo (Ful...|\n",
      "|   15183|    79|4.4301085|1975|      MASH: Season 4|\n",
      "|   15836|    79|4.4262037|1979|      MASH: Season 8|\n",
      "|    2290|    79|4.3734174|1992|Aladdin: Platinum...|\n",
      "+--------+------+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for userId 79\n",
    "reco1.join(movies_titles_df, on='movie_Id').filter('userId = 79').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1294617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|userId|rating|\n",
      "+--------+------+------+\n",
      "|    6971|    79|   5.0|\n",
      "|    3538|    79|   5.0|\n",
      "|    3541|    79|   5.0|\n",
      "|    2660|    79|   5.0|\n",
      "|    8512|    79|   5.0|\n",
      "|   13489|    79|   5.0|\n",
      "|   13748|    79|   5.0|\n",
      "|   14185|    79|   5.0|\n",
      "|    4569|    79|   5.0|\n",
      "|    4640|    79|   5.0|\n",
      "+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.filter('userId = 79').sort('rating', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f4a6799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|userId|rating|\n",
      "+--------+------+------+\n",
      "|   14648|    79|   5.0|\n",
      "|    2913|    79|   4.0|\n",
      "|   12497|    79|   4.0|\n",
      "|    8163|    79|   3.0|\n",
      "+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df.filter('userId = 79').sort('rating', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c509ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|rating|userId|\n",
      "+--------+------+------+\n",
      "|    2913|   4.0|    79|\n",
      "|    8163|   3.0|    79|\n",
      "|   12497|   4.0|    79|\n",
      "|   14648|   5.0|    79|\n",
      "+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Recommending Movies with ALS for the user 79\n",
    "single_user = testing_df.filter(testing_df['userId']==79).select(['movie_Id','rating','userId'])\n",
    "single_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "068bc5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----------+\n",
      "|movie_Id|rating|userId|prediction|\n",
      "+--------+------+------+----------+\n",
      "|   14648|   5.0|    79| 4.4529114|\n",
      "|    2913|   4.0|    79| 3.9445221|\n",
      "|   12497|   4.0|    79|  3.583714|\n",
      "|    8163|   3.0|    79|  3.225082|\n",
      "+--------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations = model.transform(single_user)\n",
    "recomendations.orderBy('prediction',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53784311",
   "metadata": {},
   "source": [
    "# Step 3: Does your approach work for your own preferences? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d877e",
   "metadata": {},
   "source": [
    "Add myself as a new user to the data set by createing a new,unique user ID for yourself. \n",
    "Selecting some movies that I  have seen among those in the training set, and add my ratings for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27c0a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My movie ratings:\n",
      "+--------+-------+------+\n",
      "|movie_Id|user_Id|rating|\n",
      "+--------+-------+------+\n",
      "|    2959|  11111|     3|\n",
      "|    2571|  11111|     4|\n",
      "|    1207|  11111|     5|\n",
      "+--------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "my_user_id = 11111\n",
    "\n",
    "my_rated_movies = [\n",
    "     ( 2959, my_user_id, 3),\n",
    "     ( 2571, my_user_id, 4),\n",
    "     ( 1207, my_user_id,5),\n",
    "     ( 296, my_user_id, 1),\n",
    "     ( 2858, my_user_id, 5), \n",
    "     ( 1172, my_user_id, 5), \n",
    "     ( 593, my_user_id,1),\n",
    "     ( 745, my_user_id,2), \n",
    "     ( 1198,my_user_id, 4),\n",
    "     ( 6016, my_user_id, 1)    \n",
    "]\n",
    "\n",
    "my_ratings_df = sqlContext.createDataFrame(my_rated_movies, ['movie_Id', 'user_Id','rating'])\n",
    "print ('My movie ratings:')\n",
    "my_ratings_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53c5e722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|userId|rating|\n",
      "+--------+------+------+\n",
      "|    2959| 11111|   3.0|\n",
      "|    2571| 11111|   4.0|\n",
      "|    1207| 11111|   5.0|\n",
      "+--------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "training_with_my_ratings_df = training_df.union(my_ratings_df)\n",
    "\n",
    "user_data = (training_with_my_ratings_df[training_with_my_ratings_df.userId == '11111'])\n",
    "user_data.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b94541d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [5, 10, 15]) \\\n",
    "            .addGrid(als.regParam, [.05, 0.1]) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37847d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidator_fbb0e94f47ca\n"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Confirm cv was built\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f5b59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Fit cross validator to the 'train' dataset\n",
    "model_my_data = cv.fit(training_with_my_ratings_df)\n",
    "\n",
    "#Extract best model from the cv model above for the new added data set\n",
    "best_model_data = model_my_data.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2630a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training data with added data is: 0.7682813125220835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11480:=================================================> (194 + 6) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for training data with added data is: 0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#evaluate the new added data training set\n",
    "train_predictions_data = best_model_data.transform(training_with_my_ratings_df)\n",
    "RMSE_train_data = evaluator.evaluate(train_predictions_data)\n",
    "print(\"RMSE for training data with added data is:\", RMSE_train_data)\n",
    "\n",
    "mae_train_data = evaluator.evaluate(train_predictions_data, {evaluator.metricName: \"mae\"})\n",
    "print(\"MAE for training data with added data is: %.3f\" % mae_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cfed5dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for testing data with model trained on added data is: 0.8375366920102414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11540:===================================>               (141 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for testing data with model trained on added data is:: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#evaluate test set from the model built on new added training data\n",
    "test_predictions_data = best_model_data.transform(testing_df)\n",
    "RMSE_data = evaluator.evaluate(test_predictions_data)\n",
    "print(\"RMSE for testing data with model trained on added data is:\", RMSE_data)\n",
    "\n",
    "mae_data = evaluator.evaluate(test_predictions_data, {evaluator.metricName: \"mae\"})\n",
    "print(\"MAE for testing data with model trained on added data is:: %.3f\" % mae_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3965858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+\n",
      "|movie_Id| userId|rating|prediction|\n",
      "+--------+-------+------+----------+\n",
      "|      28|2358799|   3.0|  3.798907|\n",
      "|     156| 973051|   5.0|  4.303177|\n",
      "|     851|1189060|   3.0|  3.399252|\n",
      "|    1100|2376892|   2.0| 2.2637596|\n",
      "|    1123|1628484|   3.0| 3.4634414|\n",
      "|    1289|1552084|   3.0| 3.4370534|\n",
      "|    1744|2376892|   5.0|  3.907899|\n",
      "|    1851| 675056|   4.0| 3.6191602|\n",
      "|    1983|2376892|   4.0| 3.1283734|\n",
      "|    1983|2629660|   3.0|  2.908888|\n",
      "+--------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#list to 10 predictions with added data\n",
    "test_predictions_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d0c4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11623:================================================>   (93 + 7) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|  3039|[[1172, 5.317329]...|\n",
      "|  3694|[[1172, 6.6968307...|\n",
      "|  4247|[[1172, 7.3059635...|\n",
      "|  7601|[[1172, 6.402927]...|\n",
      "|  8095|[[1207, 7.9479885...|\n",
      "|  8135|[[2858, 11.406401...|\n",
      "|  9399|[[1172, 7.9214673...|\n",
      "| 10897|[[1207, 6.5118833...|\n",
      "| 11215|[[1207, 7.7310195...|\n",
      "| 11430|[[2858, 10.239397...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reco_data = best_model_data.recommendForAllUsers(10)\n",
    "reco_data.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec5033df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12153:===============================================>    (92 + 7) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|userId|movie_Id|   rating|\n",
      "+------+--------+---------+\n",
      "|  3039|    1207| 5.317329|\n",
      "|  3039|    1172| 5.317329|\n",
      "|  3039|    2858| 5.317329|\n",
      "|  3039|    7016|4.5633254|\n",
      "|  3039|    3151| 4.405146|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.functions import col, avg, when, count\n",
    "\n",
    "reco1_data = reco_data\\\n",
    "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "    .select('userId', col(\"rec_exp.movie_Id\"), col(\"rec_exp.rating\"))\n",
    "\n",
    "reco1_data.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e13d449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12019:=================================================>  (96 + 4) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----+--------------------+\n",
      "|movie_Id|userId|   rating|year|               title|\n",
      "+--------+------+---------+----+--------------------+\n",
      "|    1207| 11111|4.9714956|1962|Experiment in Terror|\n",
      "|    2858| 11111|4.9714956|2000|Bounce: Bonus Mat...|\n",
      "|    1172| 11111|4.9714956|1998| Krippendorf's Tribe|\n",
      "|    1198| 11111|3.9771967|1971|The Cat O'Nine Tails|\n",
      "|    2571| 11111|3.9771967|2002|Woodrow Wilson: A...|\n",
      "+--------+------+---------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# recommendations for me \n",
    "reco1_data.join(movies_titles_df, on='movie_Id').filter('userId = 11111').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c86e8d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|userId|rating|\n",
      "+--------+------+------+\n",
      "|    2959| 11111|   3.0|\n",
      "|    2571| 11111|   4.0|\n",
      "|    1207| 11111|   5.0|\n",
      "|     296| 11111|   1.0|\n",
      "|    2858| 11111|   5.0|\n",
      "+--------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Movies for myself in the training data\n",
    "single_user_data= (training_with_my_ratings_df[training_with_my_ratings_df.userId == '11111'])\n",
    "single_user_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7944dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----------+\n",
      "|movie_Id|userId|rating|prediction|\n",
      "+--------+------+------+----------+\n",
      "|    1172| 11111|   5.0| 4.9714956|\n",
      "|    1207| 11111|   5.0| 4.9714956|\n",
      "|    2858| 11111|   5.0| 4.9714956|\n",
      "|    2571| 11111|   4.0| 3.9771967|\n",
      "|    1198| 11111|   4.0| 3.9771967|\n",
      "+--------+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predicted movies based on ALS for me\n",
    "recomendations_data = model_my_data.transform(single_user_data)\n",
    "recomendations_data.orderBy('prediction',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277b3cb",
   "metadata": {},
   "source": [
    "Top 5 movies titles recommended to me by ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b96f6e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----------+----+--------------------+\n",
      "|movie_Id|userId|rating|prediction|year|               title|\n",
      "+--------+------+------+----------+----+--------------------+\n",
      "|    2959| 11111|   3.0| 2.9828975|1991|Legend of the Dra...|\n",
      "|    2571| 11111|   4.0| 3.9771967|2002|Woodrow Wilson: A...|\n",
      "|    1207| 11111|   5.0| 4.9714956|1962|Experiment in Terror|\n",
      "|     296| 11111|   1.0| 0.9942992|2000|In His Life: The ...|\n",
      "|    2858| 11111|   5.0| 4.9714956|2000|Bounce: Bonus Mat...|\n",
      "+--------+------+------+----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations_data.join(movies_titles_df, on='movie_Id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad7ce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d0643e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_training_curve(train,rank,regular_param, iterations):\n",
    "    errors = []\n",
    "    for num_iters in range(1,iterations):\n",
    "        model = ALS(rank = rank, maxIter = num_iters, regParam = regular_param, userCol = 'userId', itemCol = 'movie_Id', nonnegative = True ).fit(train)\n",
    "        predictions = model.transform(train).fillna(0)\n",
    "        condition = [train.userId == predictions.userId,  train.movie_Id == predictions.movie_Id]\n",
    "        error = predictions.join(train,condition).select(predictions.prediction,train.rating).rdd.map(lambda x: (x[0]-x[1])**2).mean()\n",
    "        errors.append(math.sqrt(error))\n",
    "        predictions.select(predictions.userId,predictions.movie_Id,predictions.prediction).withColumnRenamed(\"prediction\",\"rating\")\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af0232ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#use the best model parameters\n",
    "errors = get_training_curve(training_df,rank=15,regular_param=0.5,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e9009c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum training error 1.0025921288190505\n"
     ]
    }
   ],
   "source": [
    "#get trainings set min error: Unable to plot learing curve as matpotlib is not working on pyspark\n",
    "print(\"minimum training error\", min(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b4f84d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8371780505884567"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_predictions error\n",
    "test_error = math.sqrt(test_predictions.rdd.map(lambda x: (x.rating-x.prediction)**2).mean())\n",
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56032b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
