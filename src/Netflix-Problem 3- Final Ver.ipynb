{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6d12a9",
   "metadata": {},
   "source": [
    "# Problem 3: Collaborative Filtering Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047e012",
   "metadata": {},
   "source": [
    "# Spark ALS based colloborative filtering model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc27d9",
   "metadata": {},
   "source": [
    "Get the dataset from s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97d1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dbfs_dir = 's3://netflixdata-yr/'\n",
    "movies_filename = dbfs_dir + 'movie_titles.txt'\n",
    "testing_filename = dbfs_dir + 'TestingRatings.txt'\n",
    "training_filename = dbfs_dir + 'TrainingRatings.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706a2dd",
   "metadata": {},
   "source": [
    "Define the schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b274678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "movies_df_schema = StructType(\n",
    "  [StructField('movie_Id', IntegerType()),\n",
    "   StructField('year', IntegerType()),\n",
    "   StructField('title', StringType())]\n",
    ")\n",
    "\n",
    "testing_df_schema = StructType(\n",
    "  [StructField('movie_Id', IntegerType()),\n",
    "   StructField('userId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")\n",
    "\n",
    "training_df_schema = StructType(\n",
    "  [StructField('movie_Id', IntegerType()),\n",
    "   StructField('userId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9252",
   "metadata": {},
   "source": [
    "Load and cache the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7c79b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movie_Id: int, userId: int, rating: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "movies_titles_df = sqlContext.read.options(header=False, inferSchema=False).schema(movies_df_schema).csv(\"s3://netflixdata-yr/movie_titles.txt\")\n",
    "testing_df = sqlContext.read.options(header=False, inferSchema=False).schema(testing_df_schema).csv(\"s3://netflixdata-yr/TestingRatings.txt\")\n",
    "training_df = sqlContext.read.options(header=False, inferSchema=False).schema(testing_df_schema).csv(\"s3://netflixdata-yr/TrainingRatings.txt\")\n",
    "\n",
    "movies_titles_df.cache()\n",
    "testing_df.cache()\n",
    "training_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e370710",
   "metadata": {},
   "source": [
    " Merge movie titles dataframe with testing and training dataframe and drop the duplicate column movieID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfe333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_df = movies_titles_df.join(testing_ratings_df, movies_titles_df[\"movie_Id\"]==testing_ratings_df[\"movieId\"])\n",
    "# testing_df = testing_df.drop(\"movieId\")\n",
    "# print(testing_df.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf2e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df = movies_titles_df.join(training_ratings_df, movies_titles_df[\"movie_Id\"]==training_ratings_df[\"movieId\"])\n",
    "# training_df = training_df.drop(\"movieId\")\n",
    "# print(training_df.show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8ed11",
   "metadata": {},
   "source": [
    "Creating temp view for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64eedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import pyspark\n",
    "sqlContext = pyspark.SQLContext(sc)  \n",
    "testing_df.createOrReplaceTempView('testing_df')\n",
    "training_df.createOrReplaceTempView('training_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88714a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required functions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03588890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALS"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movie_Id\", \n",
    "          ratingCol=\"rating\", nonnegative = True, implicitPrefs = False, coldStartStrategy=\"drop\")\n",
    "type(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5543a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  9\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Adding hyperparameters values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [5,10,15]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, 0.1]) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9303f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidator_a3ead770949b\n"
     ]
    }
   ],
   "source": [
    "# Cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8c6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Fit cross validator to the training dataset\n",
    "model = cv.fit(training_df)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc043b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.recommendation.ALSModel'>\n",
      "**Best Model**\n",
      "  Rank: 15\n",
      "  MaxIter: 10\n",
      "  RegParam: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Print best_model &  ALS model parameters\n",
    "print(type(best_model))\n",
    "print(\"**Best Model**\")\n",
    "print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
    "print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23167f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training data is: 0.767821286433335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8306:===============================================>    (182 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for training data is: 0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train_predictions = best_model.transform(training_df)\n",
    "# RMSE_train = evaluator.evaluate(train_predictions)\n",
    "# print(\"RMSE for training data is:\", RMSE_train)\n",
    "\n",
    "# mae_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mae\"})\n",
    "# print(\"MAE for training data is: %.3f\" % mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f3fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for testing data is: 0.8371780508585032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3984:========================================>           (156 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for testing data is:: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate test set predictions and evaluate using RMSE & MAE\n",
    "test_predictions = best_model.transform(testing_df)\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(\"RMSE for testing data is:\", RMSE)\n",
    "\n",
    "mae = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mae\"})\n",
    "print(\"MAE for testing data is:: %.3f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b91b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+\n",
      "|movie_Id| userId|rating|prediction|\n",
      "+--------+-------+------+----------+\n",
      "|      28|2358799|   3.0| 3.9407172|\n",
      "|     156| 973051|   5.0| 4.0387187|\n",
      "|     851|1189060|   3.0| 3.4920712|\n",
      "|    1100|2376892|   2.0| 2.2084737|\n",
      "|    1123|1628484|   3.0| 3.4529614|\n",
      "|    1289|1552084|   3.0| 3.5745318|\n",
      "|    1744|2376892|   5.0| 3.7657683|\n",
      "|    1851| 675056|   4.0| 3.5719743|\n",
      "|    1983|2376892|   4.0|  3.174658|\n",
      "|    1983|2629660|   3.0| 2.8910725|\n",
      "+--------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view top 10 test prediction movies\n",
    "test_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72400f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4223:===================================================> (98 + 2) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   481|[[6991, 5.7360873...|\n",
      "|  2678|[[12125, 4.468851...|\n",
      "|  3595|[[12293, 4.680737...|\n",
      "|  6460|[[12232, 4.913296...|\n",
      "|  7284|[[12544, 5.472037...|\n",
      "|  7576|[[15557, 5.109460...|\n",
      "|  9597|[[12952, 4.429915...|\n",
      "| 15191|[[14283, 4.873109...|\n",
      "| 15846|[[11284, 5.180236...|\n",
      "| 20461|[[2939, 5.0058455...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Recommendations for all users and list top 10\n",
    "reco = best_model.recommendForAllUsers(10)\n",
    "reco.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c24fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4301:===================================================> (98 + 2) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|userId|movie_Id|   rating|\n",
      "+------+--------+---------+\n",
      "|   481|    6991|5.7360873|\n",
      "|   481|   14648| 5.140416|\n",
      "|   481|   10743| 5.124085|\n",
      "|   481|   14361| 5.055481|\n",
      "|   481|    7569| 5.051366|\n",
      "|   481|   12952|5.0289598|\n",
      "|   481|     634| 4.975964|\n",
      "|   481|    4238|4.9639053|\n",
      "|   481|   15567|  4.94801|\n",
      "|   481|   10947| 4.940893|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.functions import col, avg, when, count\n",
    "#Unclean Recommendation Output\n",
    "reco1 = reco\\\n",
    "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "    .select('userId', col(\"rec_exp.movie_Id\"), col(\"rec_exp.rating\"))\n",
    "reco1.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c232875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----+--------------------+\n",
      "|movie_Id|userId|   rating|year|               title|\n",
      "+--------+------+---------+----+--------------------+\n",
      "|    6991|   481|5.7360873|2001|    A History of God|\n",
      "|   14648|   481| 5.140416|2003|Finding Nemo (Ful...|\n",
      "|   10743|   481| 5.124085|2001|Pearl Jam: Tourin...|\n",
      "|   14361|   481| 5.055481|1999|SpongeBob SquareP...|\n",
      "|    7569|   481| 5.051366|2004|Dead Like Me: Sea...|\n",
      "|   12952|   481|5.0289598|2005|The God Who Wasn'...|\n",
      "|     634|   481| 4.975964|1989|Christmas with Th...|\n",
      "|    4238|   481|4.9639053|2000|           Inu-Yasha|\n",
      "|   15567|   481|  4.94801|1987|Grateful Dead: Ti...|\n",
      "|   10947|   481| 4.940893|2004|     The Incredibles|\n",
      "+--------+------+---------+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view the recommendation with movie titles\n",
    "reco1.join(movies_titles_df, on='movie_Id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66de79",
   "metadata": {},
   "source": [
    "Predict recommendations for a single user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1294617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+--------------------+\n",
      "|movie_Id|userId|rating|year|               title|\n",
      "+--------+------+------+----+--------------------+\n",
      "|    2660|    79|   5.0|1989|When Harry Met Sally|\n",
      "|    3538|    79|   5.0|1988|             Beaches|\n",
      "|    3541|    79|   5.0|1981|History of the Wo...|\n",
      "|    6971|    79|   5.0|1986|Ferris Bueller's ...|\n",
      "|    8512|    79|   5.0|1999|The World Is Not ...|\n",
      "|   13489|    79|   5.0|2000|              Attila|\n",
      "|   13748|    79|   5.0|1996|The First Wives Club|\n",
      "|   14185|    79|   5.0|1964|        Mary Poppins|\n",
      "|    4569|    79|   5.0|1992|            3 Ninjas|\n",
      "|    4640|    79|   5.0|1988|            Rain Man|\n",
      "+--------+------+------+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_79 = training_df.filter('userId = 79').sort('rating', ascending=False)\n",
    "user_79.join(movies_titles_df, on='movie_Id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f4a6799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|userId|rating|\n",
      "+--------+------+------+\n",
      "|   14648|    79|   5.0|\n",
      "|    2913|    79|   4.0|\n",
      "|   12497|    79|   4.0|\n",
      "|    8163|    79|   3.0|\n",
      "+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df.filter('userId = 79').sort('rating', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c509ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+--------------------+\n",
      "|movie_Id|rating|userId|year|               title|\n",
      "+--------+------+------+----+--------------------+\n",
      "|    2913|   4.0|    79|2004|   Finding Neverland|\n",
      "|    8163|   3.0|    79|2004|        Two Brothers|\n",
      "|   12497|   4.0|    79|2000|         Bring It On|\n",
      "|   14648|   5.0|    79|2003|Finding Nemo (Ful...|\n",
      "+--------+------+------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Recommending Movies with ALS for the user 79\n",
    "single_user = testing_df.filter(testing_df['userId']==79).select(['movie_Id','rating','userId'])\n",
    "single_user.join(movies_titles_df, on='movie_Id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "068bc5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----------+----+--------------------+\n",
      "|movie_Id|rating|userId|prediction|year|               title|\n",
      "+--------+------+------+----------+----+--------------------+\n",
      "|   14648|   5.0|    79| 4.4529114|2003|Finding Nemo (Ful...|\n",
      "|    2913|   4.0|    79| 3.9445221|2004|   Finding Neverland|\n",
      "|   12497|   4.0|    79| 3.5837138|2000|         Bring It On|\n",
      "|    8163|   3.0|    79|  3.225082|2004|        Two Brothers|\n",
      "+--------+------+------+----------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations = model.transform(single_user)\n",
    "reco_user_79 =recomendations.orderBy('prediction',ascending=False)\n",
    "reco_user_79.join(movies_titles_df, on='movie_Id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53784311",
   "metadata": {},
   "source": [
    "# Step 3: Does your approach work for your own preferences? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d877e",
   "metadata": {},
   "source": [
    "# My Movie Ratings\n",
    "Add self as a new user to the data set by creating a new unique user ID for self - here it is 11111\n",
    "Selecting some movies that I have seen among those in the training set, and add my ratings for those and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27c0a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My movie ratings:\n",
      "+--------+-------+------+\n",
      "|movie_Id|user_Id|rating|\n",
      "+--------+-------+------+\n",
      "|    2959|  11111|     3|\n",
      "|    2571|  11111|     4|\n",
      "|    1207|  11111|     5|\n",
      "+--------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "my_user_id = 11111\n",
    "\n",
    "my_rated_movies = [\n",
    "     ( 2959, my_user_id, 3),\n",
    "     ( 2571, my_user_id, 4),\n",
    "     ( 1207, my_user_id,5),\n",
    "     ( 296, my_user_id, 1),\n",
    "     ( 2858, my_user_id, 5), \n",
    "     ( 1172, my_user_id, 5), \n",
    "     ( 593, my_user_id,1),\n",
    "     ( 745, my_user_id,2), \n",
    "     ( 1198,my_user_id, 4),\n",
    "     ( 6016, my_user_id, 1)    \n",
    "]\n",
    "\n",
    "my_ratings_df = sqlContext.createDataFrame(my_rated_movies, ['movie_Id', 'user_Id','rating'])\n",
    "print ('My movie ratings:')\n",
    "my_ratings_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5428be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add My Movies to Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53c5e722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|movie_Id|userId|rating|\n",
      "+--------+------+------+\n",
      "|    2959| 11111|   3.0|\n",
      "|    2571| 11111|   4.0|\n",
      "|    1207| 11111|   5.0|\n",
      "+--------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "training_with_my_ratings_df = training_df.union(my_ratings_df)\n",
    "\n",
    "user_data = (training_with_my_ratings_df[training_with_my_ratings_df.userId == '11111'])\n",
    "user_data.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b94541d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num models to be tested:  6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [5, 10, 15]) \\\n",
    "            .addGrid(als.regParam, [.05, 0.1]) \\\n",
    "            .build()\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37847d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidator_83b0618e5bf2\n"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fc774e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Model with My Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f5b59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#cross validate to the new training dataset with added data\n",
    "model_my_data = cv.fit(training_with_my_ratings_df)\n",
    "\n",
    "#Extract best model from the cv model above for the new added data \n",
    "best_model_data = model_my_data.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab517034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.recommendation.ALSModel'>\n",
      "**Best Model**\n",
      "  Rank: 15\n",
      "  MaxIter: 10\n",
      "  RegParam: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Print best_model &  ALS model parameters for the new added data set\n",
    "print(type(best_model))\n",
    "print(\"**Best Model**\")\n",
    "print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
    "print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2630a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training data with added data is: 0.7682813125220835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11480:=================================================> (194 + 6) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for training data with added data is: 0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# #evaluate the new added data training set\n",
    "# train_predictions_data = best_model_data.transform(training_with_my_ratings_df)\n",
    "# RMSE_train_data = evaluator.evaluate(train_predictions_data)\n",
    "# print(\"RMSE for training data with added data is:\", RMSE_train_data)\n",
    "\n",
    "# mae_train_data = evaluator.evaluate(train_predictions_data, {evaluator.metricName: \"mae\"})\n",
    "# print(\"MAE for training data with added data is: %.3f\" % mae_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a84fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RMSE for the New Model with my Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfed5dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for testing data with model trained on added data is: 0.8375366933337033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7331:====================================>               (139 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for testing data with model trained on added data is:: 0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#evaluate test set from the model built on new added training data\n",
    "test_predictions_data = best_model_data.transform(testing_df)\n",
    "RMSE_data = evaluator.evaluate(test_predictions_data)\n",
    "print(\"RMSE for testing data with model trained on added data is:\", RMSE_data)\n",
    "\n",
    "mae_data = evaluator.evaluate(test_predictions_data, {evaluator.metricName: \"mae\"})\n",
    "print(\"MAE for testing data with model trained on added data is:: %.3f\" % mae_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3965858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+\n",
      "|movie_Id| userId|rating|prediction|\n",
      "+--------+-------+------+----------+\n",
      "|      28|2358799|   3.0|  3.798907|\n",
      "|     156| 973051|   5.0|  4.303177|\n",
      "|     851|1189060|   3.0| 3.3992517|\n",
      "|    1100|2376892|   2.0| 2.2637594|\n",
      "|    1123|1628484|   3.0| 3.4634411|\n",
      "+--------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#list to 5 predictions with added data\n",
    "test_predictions_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d0c4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7414:================================================>    (92 + 8) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|  3039|[[2858, 5.317329]...|\n",
      "|  3694|[[1172, 6.69683],...|\n",
      "|  4247|[[1172, 7.3059635...|\n",
      "|  7601|[[1172, 6.402927]...|\n",
      "|  8095|[[1207, 7.9479885...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reco_data = best_model_data.recommendForAllUsers(10)\n",
    "reco_data.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec5033df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7492:====================================================>(99 + 1) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|userId|movie_Id|   rating|\n",
      "+------+--------+---------+\n",
      "|  3039|    1172| 5.317329|\n",
      "|  3039|    1207| 5.317329|\n",
      "|  3039|    2858| 5.317329|\n",
      "|  3039|    7016| 4.563325|\n",
      "|  3039|    3151|4.4051466|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.functions import col, avg, when, count\n",
    "\n",
    "reco1_data = reco_data\\\n",
    "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "    .select('userId', col(\"rec_exp.movie_Id\"), col(\"rec_exp.rating\"))\n",
    "\n",
    "reco1_data.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e13d449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7700:============>(94 + 6) / 100][Stage 7701:>               (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----+--------------------+\n",
      "|movie_Id|userId|   rating|year|               title|\n",
      "+--------+------+---------+----+--------------------+\n",
      "|    2858| 11111|4.9714956|2000|Bounce: Bonus Mat...|\n",
      "|    1207| 11111|4.9714956|1962|Experiment in Terror|\n",
      "|    1172| 11111|4.9714956|1998| Krippendorf's Tribe|\n",
      "|    2571| 11111|3.9771967|2002|Woodrow Wilson: A...|\n",
      "|    1198| 11111|3.9771967|1971|The Cat O'Nine Tails|\n",
      "+--------+------+---------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# recommendations for me \n",
    "reco1_data.join(movies_titles_df, on='movie_Id').filter('userId = 11111').sort(desc('rating')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c86e8d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+--------------------+\n",
      "|movie_Id|userId|rating|year|               title|\n",
      "+--------+------+------+----+--------------------+\n",
      "|    2858| 11111|   5.0|2000|Bounce: Bonus Mat...|\n",
      "|    1207| 11111|   5.0|1962|Experiment in Terror|\n",
      "|    1172| 11111|   5.0|1998| Krippendorf's Tribe|\n",
      "|    2571| 11111|   4.0|2002|Woodrow Wilson: A...|\n",
      "|    1198| 11111|   4.0|1971|The Cat O'Nine Tails|\n",
      "+--------+------+------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Movies for myself in the training data\n",
    "single_user_data= (training_with_my_ratings_df[training_with_my_ratings_df.userId == '11111'])\n",
    "single_user_data.join(movies_titles_df, on='movie_Id').filter('userId = 11111').sort(desc('rating')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7944dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----------+\n",
      "|movie_Id|userId|rating|prediction|\n",
      "+--------+------+------+----------+\n",
      "|    2858| 11111|   5.0| 4.9714956|\n",
      "|    1207| 11111|   5.0| 4.9714956|\n",
      "|    1172| 11111|   5.0| 4.9714956|\n",
      "|    2571| 11111|   4.0| 3.9771967|\n",
      "|    1198| 11111|   4.0| 3.9771967|\n",
      "+--------+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predicted movies based on ALS for me\n",
    "recomendations_data = model_my_data.transform(single_user_data)\n",
    "recomendations_data.orderBy('prediction',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277b3cb",
   "metadata": {},
   "source": [
    "Top movies titles recommended to me by ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b96f6e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----+--------------------+\n",
      "|movie_Id|userId|   rating|year|               title|\n",
      "+--------+------+---------+----+--------------------+\n",
      "|    3941| 11111|1.6965362|1975|The French Connec...|\n",
      "|   13878| 11111|1.7531966|1975|Sanford and Son: ...|\n",
      "|    5369| 11111| 1.787698|1972|Sanford and Son: ...|\n",
      "+--------+------+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recomendations_data.join(movies_titles_df, on='movie_Id').show(5)\n",
    "array = [2959,2571,1207,296, 2858, 1172, 593, 745, 1198, 6016]  \n",
    "top_reco =reco1_data.join(movies_titles_df, on='movie_Id').filter('userId = 11111').sort(desc('rating'))\n",
    "top_reco_yr =top_reco.filter(top_reco.movie_Id.isin(array) == False)\n",
    "top_reco_yr.orderBy('rating',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad7ce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d0643e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_training_curve(train,rank,regular_param, iterations):\n",
    "    errors = []\n",
    "    for num_iters in range(1,iterations):\n",
    "        model = ALS(rank = rank, maxIter = num_iters, regParam = regular_param, userCol = 'userId', itemCol = 'movie_Id', nonnegative = True ).fit(train)\n",
    "        predictions = model.transform(train).fillna(0)\n",
    "        condition = [train.userId == predictions.userId,  train.movie_Id == predictions.movie_Id]\n",
    "        error = predictions.join(train,condition).select(predictions.prediction,train.rating).rdd.map(lambda x: (x[0]-x[1])**2).mean()\n",
    "        errors.append(math.sqrt(error))\n",
    "        predictions.select(predictions.userId,predictions.movie_Id,predictions.prediction).withColumnRenamed(\"prediction\",\"rating\")\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af0232ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#use the best model parameters\n",
    "errors = get_training_curve(training_df,rank=15,regular_param=0.5,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e9009c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum training error 1.0025921288190505\n"
     ]
    }
   ],
   "source": [
    "#get trainings set min error: Unable to plot learing curve as matpotlib is not working on pyspark\n",
    "print(\"minimum training error\", min(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b4f84d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8371780505884567"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_predictions error\n",
    "test_error = math.sqrt(test_predictions.rdd.map(lambda x: (x.rating-x.prediction)**2).mean())\n",
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56032b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
